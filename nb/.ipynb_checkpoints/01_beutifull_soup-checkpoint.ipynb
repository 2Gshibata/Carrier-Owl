{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "- beutifull soup を使ってarchive からデータを取得する\n",
    "- way\n",
    "    1. arxiv のcomputer science のrescent のページを取得\n",
    "    1. 前日の日付だけ抽出\n",
    "    1. それぞれの論文ページにアクセスし、タイトル、アブストラクトからキーワードを検索する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import everything I need :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from fastprogress import progress_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step1\n",
    "- way\n",
    "    1. arxiv のcomputer science のrescent のページを取得　　　<----- here !!\n",
    "    1. 今日の日付だけ抽出\n",
    "    1. それぞれの論文ページにアクセスし、タイトル、アブストラクトからキーワードを検索する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://arxiv.org/list/cs/pastweek?show=10000'\n",
    "response = requests.get(url)\n",
    "html = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step2\n",
    "- way\n",
    "    1. arxiv のcomputer science のrescent のページを取得\n",
    "    1. 今日の日付だけ抽出　　　<----- here!!\n",
    "    1. それぞれの論文ページにアクセスし、タイトル、アブストラクトからキーワードを検索する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "today_html = html.split('2019</h3>')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---> なにしてるかわからない場合は、htmlを表示して、`2019</h3>`の部分を見ればわかると思う"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step3\n",
    "- way\n",
    "    1. arxiv のcomputer science のrescent のページを取得\n",
    "    1. 今日の日付だけ抽出\n",
    "    1. それぞれの論文ページにアクセスし、タイトル、アブストラクトからキーワードを検索する　　<----- here!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = BeautifulSoup(today_html)\n",
    "id_list = bs.find_all(class_='list-identifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "例として、abstract内に `CNN` が入っている場合表示する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='203' class='' max='203', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [203/203 03:21<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: Explanation based Handwriting Verification\n",
      "Deep learning system have drawback that their output is not accompanied with\n",
      "ex-planation. In a domain such as forensic handwriting verification it is\n",
      "essential to provideexplanation to jurors. The goal of handwriting verification\n",
      "is to find a measure of confi-dence whether the given handwritten samples are\n",
      "written by the same or different writer.We propose a method to generate\n",
      "explanations for the confidence provided by convolu-tional neural network (CNN)\n",
      "which maps the input image to 15 annotations (features)provided by experts. Our\n",
      "system comprises of: (1) Feature learning network (FLN),a differentiable\n",
      "system, (2) Inference module for providing explanations. Furthermore,inference\n",
      "module provides two types of explanations: (a) Based on cosine\n",
      "similaritybetween categorical probabilities of each feature, (b) Based on\n",
      "Log-Likelihood Ratio(LLR) using directed probabilistic graphical model. We\n",
      "perform experiments using acombination of feature learning network (FLN) and\n",
      "each inference module. We evaluateour system using XAI-AND dataset, containing\n",
      "13700 handwritten samples and 15 cor-responding expert examined features for\n",
      "each sample. The dataset is released for publicuse and the methods can be\n",
      "extended to provide explanations on other verification taskslike face\n",
      "verification and bio-medical comparison. This dataset can serve as the basis\n",
      "and benchmark for future research in explanation based handwriting\n",
      "verification. The code is available on github.\n",
      "\n",
      "title: Harnessing the Power of Deep Learning Methods in Healthcare: Neonatal Pain Assessment from Crying Sound\n",
      "Neonatal pain assessment in clinical environments is challenging as it is\n",
      "discontinuous and biased. Facial/body occlusion can occur in such settings due\n",
      "to clinical condition, developmental delays, prone position, or other external\n",
      "factors. In such cases, crying sound can be used to effectively assess neonatal\n",
      "pain. In this paper, we investigate the use of a novel CNN architecture (N-CNN)\n",
      "along with other CNN architectures (VGG16 and ResNet50) for assessing pain from\n",
      "crying sounds of neonates. The experimental results demonstrate that using our\n",
      "novel N-CNN for assessing pain from the sounds of neonates has a strong\n",
      "clinical potential and provides a viable alternative to the current assessment\n",
      "practice.\n",
      "\n",
      "title: FreeAnchor: Learning to Match Anchors for Visual Object Detection\n",
      "Modern CNN-based object detectors assign anchors for ground-truth objects\n",
      "under the restriction of object-anchor Intersection-over-Unit (IoU). In this\n",
      "study, we propose a learning-to-match approach to break IoU restriction,\n",
      "allowing objects to match anchors in a flexible manner. Our approach, referred\n",
      "to as FreeAnchor, updates hand-crafted anchor assignment to \"free\" anchor\n",
      "matching by formulating detector training as a maximum likelihood estimation\n",
      "(MLE) procedure. FreeAnchor targets at learning features which best explain a\n",
      "class of objects in terms of both classification and localization. FreeAnchor\n",
      "is implemented by optimizing detection customized likelihood and can be fused\n",
      "with CNN-based detectors in a plug-and-play manner. Experiments on MS-COCO\n",
      "demonstrate that FreeAnchor consistently outperforms their counterparts with\n",
      "significant margins.\n",
      "\n",
      "title: Semantic-Aware Scene Recognition\n",
      "Scene recognition is currently one of the top-challenging research fields in\n",
      "computer vision. This may be due to the ambiguity between classes: images of\n",
      "several scene classes may share similar objects, which causes confusion among\n",
      "them. The problem is aggravated when images of a particular scene class are\n",
      "notably different. Convolutional Neural Networks (CNNs) have significantly\n",
      "boosted performance in scene recognition, albeit it is still far below from\n",
      "other recognition tasks (e.g., object or image recognition). In this paper, we\n",
      "describe a novel approach for scene recognition based on an end-to-end\n",
      "multi-modal CNN that combines image and context information by means of an\n",
      "attention module. Context information, in the shape of semantic segmentation,\n",
      "is used to gate features extracted from the RGB image by leveraging on\n",
      "information encoded in the semantic representation: the set of scene objects\n",
      "and stuff, and their relative locations. This gating process reinforces the\n",
      "learning of indicative scene content and enhances scene disambiguation by\n",
      "refocusing the receptive fields of the CNN towards them. Experimental results\n",
      "on four publicly available datasets show that the proposed approach outperforms\n",
      "every other state-of-the-art method while significantly reducing the number of\n",
      "network parameters. All the code and data used along this paper is available at\n",
      "https://github.com/vpulab/Semantic-Aware-Scene-Recognition\n",
      "\n",
      "title: Utilizing Temporal Information in DeepConvolutional Network for Efficient Soccer BallDetection and Tracking\n",
      "Soccer ball detection is identified as one of the critical challenges in the\n",
      "RoboCup competition. It requires an efficient vision system capable of handling\n",
      "the task of detection with high precision and recall and providing robust and\n",
      "low inference time. In this work, we present a novel convolutional neural\n",
      "network (CNN) approach to detect the soccer ball in an image sequence. In\n",
      "contrast to the existing methods where only the current frame or an image is\n",
      "used for the detection, we make use of the history of frames. Using history\n",
      "allows to efficiently track the ball in situations where the ball disappears or\n",
      "gets partially occluded in some of the frames. Our approach exploits\n",
      "spatio-temporal correlation and detects the ball based on the trajectory of its\n",
      "movements. We present our results with three convolutional methods, namely\n",
      "temporal convolutional networks (TCN), ConvLSTM, and ConvGRU. We first solve\n",
      "the detection task for an image using fully convolutional encoder-decoder\n",
      "architecture, and later, we use it as an input to our temporal models and\n",
      "jointly learn the detection task in sequences of images. We evaluate all our\n",
      "experiments on a novel dataset prepared as a part of this work. Furthermore, we\n",
      "present empirical results to support the effectiveness of using the history of\n",
      "the ball in challenging scenarios.\n",
      "\n",
      "title: Powerset Convolutional Neural Networks\n",
      "We present a novel class of convolutional neural networks (CNNs) for set\n",
      "functions, i.e., data indexed with the powerset of a finite set. The\n",
      "convolutions are derived as linear, shift-equivariant functions for various\n",
      "notions of shifts on set functions. The framework is fundamentally different\n",
      "from graph convolutions based on the Laplacian, as it provides not one but\n",
      "several basic shifts, one for each element in the ground set. Prototypical\n",
      "experiments with several set function classification tasks on synthetic\n",
      "datasets and on datasets derived from real-world hypergraphs demonstrate the\n",
      "potential of our new powerset CNNs.\n",
      "\n",
      "title: Decoupled Box Proposal and Featurization with Ultrafine-Grained Semantic Labels Improve Image Captioning and Visual Question Answering\n",
      "Object detection plays an important role in current solutions to vision and\n",
      "language tasks like image captioning and visual question answering. However,\n",
      "popular models like Faster R-CNN rely on a costly process of annotating\n",
      "ground-truths for both the bounding boxes and their corresponding semantic\n",
      "labels, making it less amenable as a primitive task for transfer learning. In\n",
      "this paper, we examine the effect of decoupling box proposal and featurization\n",
      "for down-stream tasks. The key insight is that this allows us to leverage a\n",
      "large amount of labeled annotations that were previously unavailable for\n",
      "standard object detection benchmarks. Empirically, we demonstrate that this\n",
      "leads to effective transfer learning and improved image captioning and visual\n",
      "question answering models, as measured on publicly available benchmarks.\n",
      "\n",
      "title: An Entity-Driven Framework for Abstractive Summarization\n",
      "Abstractive summarization systems aim to produce more coherent and concise\n",
      "summaries than their extractive counterparts. Popular neural models have\n",
      "achieved impressive results for single-document summarization, yet their\n",
      "outputs are often incoherent and unfaithful to the input. In this paper, we\n",
      "introduce SENECA, a novel System for ENtity-drivEn Coherent Abstractive\n",
      "summarization framework that leverages entity information to generate\n",
      "informative and coherent abstracts. Our framework takes a two-step approach:\n",
      "(1) an entity-aware content selection module first identifies salient sentences\n",
      "from the input, then (2) an abstract generation module conducts cross-sentence\n",
      "information compression and abstraction to generate the final summary, which is\n",
      "trained with rewards to promote coherence, conciseness, and clarity. The two\n",
      "components are further connected using reinforcement learning. Automatic\n",
      "evaluation shows that our model significantly outperforms previous\n",
      "state-of-the-art on ROUGE and our proposed coherence measures on New York Times\n",
      "and CNN/Daily Mail datasets. Human judges further rate our system summaries as\n",
      "more informative and coherent than those by popular summarization models.\n",
      "\n",
      "title: Coherent Optical Communications Enhanced by Machine Intelligence\n",
      "Uncertainty in discriminating between different received coherent signals is\n",
      "integral to the operation of many free-space optical communications protocols,\n",
      "and is often difficult when the receiver measures a weak signal. Here we design\n",
      "an optical communications scheme that uses balanced homodyne detection in\n",
      "combination with an unsupervised generative machine learning and convolutional\n",
      "neural network (CNN) system, and demonstrate its efficacy in a realistic\n",
      "simulated coherent quadrature phase shift keyed (QPSK) communications system.\n",
      "Additionally, we program the neural network system at the transmitter such that\n",
      "it autonomously learns to correct for the noise associated with a weak QPSK\n",
      "signal, which is shared with the network state of the receiver prior to the\n",
      "implementation of the communications. We find that the scheme significantly\n",
      "reduces the overall error probability of the communications system, achieving\n",
      "the classical optimal limit. This communications design is straightforward to\n",
      "build, implement, and scale. We anticipate that these results will allow for a\n",
      "significant enhancement of current classical and quantum coherent optical\n",
      "communications technologies.\n",
      "\n",
      "title: The application of Convolutional Neural Networks to Detect Slow, Sustained Deformation in InSAR Timeseries\n",
      "Automated systems for detecting deformation in satellite InSAR imagery could\n",
      "be used to develop a global monitoring system for volcanic and urban\n",
      "environments. Here we explore the limits of a CNN for detecting slow, sustained\n",
      "deformations in wrapped interferograms. Using synthetic data, we estimate a\n",
      "detection threshold of 3.9cm for deformation signals alone, and 6.3cm when\n",
      "atmospheric artefacts are considered. Over-wrapping reduces this to 1.8cm and\n",
      "5.0cm respectively as more fringes are generated without altering SNR. We test\n",
      "the approach on timeseries of cumulative deformation from Campi Flegrei and\n",
      "Dallol, where over-wrapping improves classication performance by up to 15%. We\n",
      "propose a mean-filtering method for combining results of different wrap\n",
      "parameters to flag deformation. At Campi Flegrei, deformation of 8.5cm/yr was\n",
      "detected after 60days and at Dallol, deformation of 3.5cm/yr was detected after\n",
      "310 days. This corresponds to cumulative displacements of 3 cm and 4 cm\n",
      "consistent with estimates based on synthetic data.\n",
      "\n",
      "title: Generative Machine Learning for Robust Free-Space Communication\n",
      "Realistic free-space optical communications systems suffer from turbulent\n",
      "propagation of light through the atmosphere and detector noise at the receiver,\n",
      "which can significantly degrade the optical mode quality of the received state,\n",
      "increase cross-talk between modes, and correspondingly increase the symbol\n",
      "error ratio (SER) of the system. In order to overcome these obstacles, we\n",
      "develop a state-of-the-art generative machine learning (GML) and convolutional\n",
      "neural network (CNN) system in combination, and demonstrate its efficacy in a\n",
      "free-space optical (FSO) communications setting. The system corrects for the\n",
      "distortion effects due to turbulence and reduces detector noise, resulting in\n",
      "significantly lowered SERs and cross-talk at the output of the receiver, while\n",
      "requiring no feedback. This scheme is straightforward to scale, and may provide\n",
      "a concrete and cost effective technique to establishing long range classical\n",
      "and quantum communication links in the near future.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "key_word = 'CNN'\n",
    "articles = []\n",
    "for id_ in progress_bar(id_list):\n",
    "    a = id_.find('a')\n",
    "    _url = a.get('href')\n",
    "    url = 'https://arxiv.org'+_url\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "\n",
    "    bs = BeautifulSoup(html)\n",
    "    title    = bs.find('meta', attrs={'property': 'og:title'})['content']\n",
    "    abstract = bs.find('meta', attrs={'property': 'og:description'})['content']\n",
    "    if key_word in abstract:\n",
    "        articles.append({'url': url, 'title': title, 'abstract': abstract})\n",
    "        print(f'title: {title}')\n",
    "        print(abstract)\n",
    "        print('')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
